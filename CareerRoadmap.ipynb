{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916cb069",
   "metadata": {},
   "source": [
    "**OpenAi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc70ea50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "Openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f721181",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TUF_Gaming\\AppData\\Local\\Temp\\ipykernel_32320\\3901251041.py:3: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(temperature=0.7)\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09d323f",
   "metadata": {},
   "source": [
    "**Hugging face- LLaMa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed112474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "HUGGINGFACE_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=[\"lm_head\"],\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, \n",
    "    use_fast=True, \n",
    "    use_auth_token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_auth_token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "\n",
    "# Create a pipeline for text generation\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=256,\n",
    "    temperature=0.6,\n",
    "    top_k=10,\n",
    "    top_p=0.7,\n",
    "    repetition_penalty=1.2\n",
    ")\n",
    "\n",
    "# Wrap the pipeline in a LangChain-compatible LLM\n",
    "llm = HuggingFacePipeline(pipeline=generator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deaeb4b",
   "metadata": {},
   "source": [
    "Use the below part after either OpenAI and Hugging face LLM instance is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7626228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variables =['Job_role'],\n",
    "    template = \"Provide a roadmap for a beginner based on the {Job_role}.\"\n",
    "        \"Include the languages, frameworks, and packages they must learn.\" \n",
    "        \"Make it summarized limit to 128 tokens\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
    "response = chain.run(\"Data Engineer\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0afff0",
   "metadata": {},
   "source": [
    "**LLama 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62b97e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Detected: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Detected: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"No GPU detected, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34782bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.llms import CTransformers\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d575c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def getLLamaresponse(Job_role):\n",
    "\n",
    "    MODEL_PATH = \"E:\\\\AIML\\\\Roadmap\\\\Models\\\\llama-2-7b-chat.ggmlv3.q8_0.bin\"\n",
    "\n",
    "    # Load the model with GPU optimization and streaming\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        model_type='llama',\n",
    "        gpu_layers=20,  # Adjust based on VRAM. 24 is too high for 4GB with 8-bit quantization\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Prompt Template\n",
    "    template = \"\"\"\n",
    "        Provide a roadmap for a beginner based on the {Job_role}. \n",
    "        Include the languages, frameworks, and packages they must learn. Make it summarized limit to 128 tokens.\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(input_variables=[\"Job_role\"], template=template)\n",
    "    formatted_prompt = prompt.format(Job_role=Job_role)\n",
    "\n",
    "    response = \"\"\n",
    "    for text_chunk in llm(\n",
    "        formatted_prompt,\n",
    "        max_new_tokens=200,  # Limit the response length for faster output\n",
    "        temperature=0.01,  # Lower temperature for faster, more deterministic output\n",
    "        repetition_penalty=1.05,  # Slightly lower to reduce unnecessary repetition\n",
    "        stream=True  # Enable streaming for faster first response\n",
    "    ):\n",
    "    \n",
    "        print(text_chunk, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c4b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Beginner Roadmap:\n",
      "    \n",
      "    1. Learn Python: Essential language for data engineering.\n",
      "    2. Familiarize with pandas and NumPy.\n",
      "    3. Learn SQL: Data manipulation and querying.\n",
      "    4. Explore data visualization tools like Matplotlib and Seaborn.\n",
      "    5. Learn about data storage solutions like HDFS and AWS S3.\n",
      "    6. Familiarize with big data processing frameworks like Apache Spark and Apache Flink.\n",
      "    7. Learn about data governance and quality control.\n",
      "    8. Explore machine learning libraries like scikit-learn and TensorFlow.\n",
      "    9. Learn about cloud computing platforms like AWS and GCP.\n",
      "    10. Practice with real-world projects and datasets.\n",
      "    \n",
      "    Total tokens: 128"
     ]
    }
   ],
   "source": [
    "getLLamaresponse(\"Data Engineer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e964aa15",
   "metadata": {},
   "source": [
    "**QWEN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d5b705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebd31832b2d49d6a83c52b440696d6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Career Roadmap:\n",
      "\n",
      "**Programming Languages**  \n",
      "- **Python**: Core language for data processing, analysis, and scripting.  \n",
      "- **Java/Scala**: For distributed systems and big data frameworks.  \n",
      "- **C#**: For backend development and microservices.  \n",
      "- **SQL**: For querying databases and managing data storage.  \n",
      "\n",
      "**Frameworks and Libraries**  \n",
      "- **Python**: Pandas, NumPy, Scikit-learn, SQLAlchemy, and PySpark.  \n",
      "- **Java**: Apache Kafka, Hadoop, Spark, and JDBC.  \n",
      "- **C#**: Entity Framework, LINQ, and SQL Server.  \n",
      "- **Web Development**: Django, Flask, or Spring Boot.  \n",
      "\n",
      "**Tools and Platforms**  \n",
      "- **Jupyter Notebooks**: For interactive data exploration.  \n",
      "- **Git & GitHub**: Version control and collaboration.  \n",
      "- **Docker**: Containerization of applications.  \n",
      "- **Kubernetes**: For orchestration of services.  \n",
      "- **AWS/Azure/GCP**: Cloud platforms for deployment and scaling.  \n",
      "- **Apache Airflow**: For workflow automation.  \n",
      "\n",
      "**Suggested Projects**  \n",
      "1. **Data Cleaning and Analysis** – Use Python to process and analyze datasets.  \n",
      "2. **Real-time Stream Processing** – Implement Kafka and Spark for streaming data.  \n",
      "3. **ETL Pipeline** – Build a pipeline using Python and SQL.  \n",
      "4. **Microservice Architecture** – Develop a service using Java and Docker.  \n",
      "5. **Cloud-Based Data Warehouse** – Use AWS Glue and BigQuery for data warehousing.  \n",
      "6. **Machine Learning Model** – Train a model using scikit-learn and pandas.  \n",
      "\n",
      "##\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import streamlit as st\n",
    "\n",
    "MODEL_DIR = \"E:\\\\AIML\\\\Qwen3-1.7B\"  # Path to the model directory\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=[\"lm_head\"]\n",
    ")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=0\n",
    ")\n",
    "\n",
    "def get_llama_response(job_role):\n",
    "    \"\"\"Generate roadmap using Qwen3-1.7B asynchronously.\"\"\"\n",
    "\n",
    "    # Construct the chat-style prompt with thinking mode enabled\n",
    "    prompt = (\n",
    "        \"You are a career advisor. Generate a structured learning roadmap for a beginner aspiring to be a {Job_role}. \"\n",
    "        \"The response should include the following sections:\\n\"\n",
    "        \"1. Programming Languages\\n\"\n",
    "        \"2. Frameworks and Libraries\\n\"\n",
    "        \"3. Tools and Platforms\\n\"\n",
    "        \"4. Suggested Projects\\n\\n\"\n",
    "        \"Ensure clarity and conciseness. Avoid markdown formatting. \"\n",
    "        \"Start the response with '### Career Roadmap:' and end with '##'.\\n\\n\"\n",
    "        \"### Career Roadmap:\\n\"\n",
    "    ).format(Job_role=job_role)\n",
    "\n",
    "    # Construct the input message for Qwen3-1.7B\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template with thinking enabled\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=False\n",
    "    )\n",
    "\n",
    "    # Prepare model input\n",
    "    model_inputs = tokenizer([input_text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Generate response\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512,  # Adjust token limit as required\n",
    "        temperature=0.05,\n",
    "        repetition_penalty=1.1\n",
    "    )\n",
    "\n",
    "    # Extract generated output\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "    # Parse thinking content and main response\n",
    "    try:\n",
    "        # Identify the 'thinking' token (151668)\n",
    "        think_index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        think_index = 0\n",
    "\n",
    "    # thinking_content = tokenizer.decode(output_ids[:think_index], skip_special_tokens=True).strip()\n",
    "    response_content = tokenizer.decode(output_ids[think_index:], skip_special_tokens=True).strip()\n",
    "\n",
    "    return response_content\n",
    "\n",
    "print(get_llama_response('Data Engineer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa52346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
